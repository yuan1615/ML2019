---
title: "Machine Learning 2019"
author:
  - 袁欣
date: "2019年3月10日"
documentclass: ctexart
output:
  rticles::ctex:
    number_sections: yes
classoption: "hyperref,"
geometry: tmargin=2.5cm,bmargin=2.5cm,lmargin=2cm,rmargin=2cm
---

```{r eval=TRUE, echo=FALSE}
library(ggplot2)
library(easyGgplot2)
library(ggisoband)
library(MASS)
```
# 线性模型

## 西瓜数据
从csv文件中读取西瓜数据，并进行展示。 

```{r random number, eval = TRUE, echo =FALSE}
wmda <- read.csv(file = "data/西瓜数据3.0a.csv")
wmda$label <- as.factor(wmda$label)
```
* 数据展示：
```{r summary1, eval = TRUE, echo =FALSE}
knitr::kable(head(wmda))
summary(wmda[, -1])

```

* 画图：

```{r plot1, eval = TRUE, echo =FALSE}
ggplot(data = wmda, aes(x = density, y = sugar, color = label)) + 
  geom_point(size = 2.0, shape = 16)

```


## 逻辑回归

* 编程实现逻辑回归的梯度下降算法
```{r log, eval=TRUE, echo=FALSE}
sigmoid <- function(z){
  return(1 / (1 + exp(-z)))
}
gradientDescent <- function(X, y, theta, alpha, num_iters){
  # Initialize some useful values
  m <- length(y) # number of training examples
  J.history <- rep(0, num_iters)
  for(i in 1:num_iters){
    theta <- theta - (1/m) * (t(X) %*% (sigmoid(X %*% theta) - y))
    #  Save the cost J in every iteration    
    J.history[i] <- (1/m) * 
      sum(-y * log(sigmoid(X %*% theta)) - 
            (1-y) * (log(1 - sigmoid(X %*% theta))))
  }
  return(list(theta = theta, J = J.history))
}

X <- as.matrix(wmda[, 2:3])
X <- cbind(1, X)
y <- as.matrix(as.numeric(as.character(wmda$label)))
initial_theta <- matrix(rep(0, 3), ncol = 1)
alpha <- 0.1
Ret <- gradientDescent(X, y, initial_theta, alpha, 
                       num_iters = 5000)

```

* 回归参数与代价函数曲线如下：

```{r plot5, eval=TRUE, echo=FALSE}
as.numeric(Ret$theta)
ggplot(data = data.frame(item = 1:length(Ret$J), J = Ret$J),
       aes(x = item, y = J)) + geom_line()
```

* 决策边界如下：

```{r plot6, eval=TRUE, echo=FALSE}
ggplot(data = wmda, aes(x = density, y = sugar, color = label)) + 
  geom_point(size = 2.0, shape = 16) + 
  geom_abline(slope = -Ret$theta[2] / Ret$theta[3], 
              intercept = -Ret$theta[1]/Ret$theta[3], 
              color = "red")
```


### mapFeature
从上图可以看到，逻辑回归对西瓜集的分类是较差的。我们也可以直观的看到西瓜集是线性不可分的！所以这里引入了高阶特征，构建非线性边界去划分西瓜集。  
构建方法为选择最高次数，将两变量映射到高阶上形成新特征。例如构建最高幂次为6的特征，此时会产生新特征如：$x_1^6$、$x_2^6$、$x_1^5x_2$、......、$x_1x_2$、$x_2$、$x_1$共28个特征。

* 构建函数
```{r mapFeature, eval=TRUE, echo=FALSE}
mapFeature <- function(x1, x2, degree){
  df <- matrix(1, nrow = length(x1))
  for(i in 1:degree){
    for(j in 0:i){
      x <- x1^(i - j) * x2^(j)
      df <- cbind(df, x)
    }
    
  }
  return(df)
}
x1 <- wmda$density
x2 <- wmda$sugar
X <- mapFeature(x1, x2, 6)
```

### 正则化

* 编程实现正则化逻辑回归的梯度下降算法
```{r log1, eval=TRUE, echo=FALSE}
gradientDescent <- function(X, y, theta, alpha, num_iters, lambda){
  # Initialize some useful values
  m <- length(y) # number of training examples
  n <- ncol(X)
  J.history <- rep(0, num_iters)
  for(i in 1:num_iters){
    theta[1] <- theta[1] - 
      (1/m) * (t(X[, 1]) %*% (sigmoid(X %*% theta) - y))
    theta[2:n] <- theta[2:n] - 
      (1/m) * (t(X[, 2:n]) %*% (sigmoid(X %*% theta) - y)) + 
      lambda/m * theta[2:n]
    #  Save the cost J in every iteration    
    J.history[i] <- (1/m) * 
      sum(-y * log(sigmoid(X %*% theta)) - 
            (1-y) * (log(1 - sigmoid(X %*% theta)))) + 
      (lambda/2/m) * sum(theta[2:n] ^2)
  }
  return(list(theta = theta, J = J.history))
}

y <- as.matrix(as.numeric(as.character(wmda$label)))
initial_theta <- matrix(rep(0, 28), ncol = 1)
alpha <- 0.1
lambda <- 0
Ret <- gradientDescent(X, y, initial_theta, alpha, 
                       num_iters = 100000, lambda)
```

* 回归参数、预测精度、代价函数曲线如下：

```{r plot7, eval=TRUE, echo=FALSE}
as.numeric(Ret$theta)
p <- sigmoid(X %*% Ret$theta)
pos <- which(p >= 0.5)
neg <- which(p < 0.5)
p[pos] <- 1
p[neg] <- 0
t <- table(p, wmda$label)
print(paste("prediction accuracy = ", sum(t) / sum(diag(t)) * 100,
            "%"))
ggplot(data = data.frame(item = 1:length(Ret$J), J = Ret$J),
       aes(x = item, y = J)) + geom_line()
```

* 非线性决策边界如下：

```{r plot8, eval=TRUE, echo=FALSE}
x1 <- seq(0, 0.8, 0.01)
x2 <- seq(0, 0.5, 0.01)
x.grad <- data.frame()
for(i in x1){
  for(j in x2){
    x.grad <- rbind(x.grad, c(i, j))
  }
}
colnames(x.grad) <- c("x1", "x2")
X.grad <- mapFeature(x.grad[, 1], x.grad[, 2], 6) 
p <- sigmoid(X.grad %*% Ret$theta)
idx <- which(p < 0.5+0.01 & p > 0.5-0.01)

ggplot(data = wmda, aes(x = density, y = sugar, color = label)) + 
  geom_point(size = 2.0, shape = 16) + 
  geom_line(data = x.grad[idx, ], aes(x = x1, y = x2),  colour = "red")
```


### 多分类问题
可以利用One-vs-all算法，创建伪训练集，例如：
预测天气（Sunny、Cloudy、Rain、Snow）,可以学习四个逻辑回归，判断哪个概率最高，则属于哪一类。

### 利用牛顿法求解

```{r HessianMatrix, eval=TRUE, echo=FALSE}
HessianMatrix <- function(X, y, theta, num_iters){
  # Initialize some useful values
  m <- length(y) # number of training examples
  J.history <- rep(0, num_iters)
  
  for(i in 1:num_iters){
    partial1 <- (1/m) * (t(X) %*% (sigmoid(X %*% theta) - y))
    partial2 <- (1/m) * (t(X) %*% (X * as.numeric(
                           (sigmoid(X %*% theta) * 
                           (1 - sigmoid(X %*% theta))))) )
    theta <- theta - solve(partial2) %*% partial1
    #  Save the cost J in every iteration    
    J.history[i] <- (1/m) * 
      sum(-y * log(sigmoid(X %*% theta)) - 
            (1-y) * (log(1 - sigmoid(X %*% theta))))
  }
  return(list(theta = theta, J = J.history))
}
X <- as.matrix(wmda[, 2:3])
X <- cbind(1, X)
initial_theta <- matrix(rep(0, 3), ncol = 1)
Ret <- HessianMatrix(X, y, initial_theta, num_iters = 10)
```

* 回归参数、代价函数曲线、决策边界如下：
```{r plot9, eval=TRUE, echo=FALSE}
as.numeric(Ret$theta)
p1 <- ggplot(data = data.frame(item = 1:length(Ret$J), J = Ret$J),
             aes(x = item, y = J)) + geom_line()
p2 <- ggplot(data = wmda, aes(x = density, y = sugar, color = label)) + 
        geom_point(size = 2.0, shape = 16) + 
        geom_abline(slope = -Ret$theta[2] / Ret$theta[3], 
                    intercept = -Ret$theta[1]/Ret$theta[3], 
                    color = "red")
```

```{r plot10, eval=TRUE, echo=FALSE}
ggplot2.multiplot(p1, p2, cols=2)
```

### 牛顿法正则化

```{r HessianMatrix2, eval=TRUE, echo=FALSE}
HessianMatrix2 <- function(X, y, theta, num_iters, lambda){
  # Initialize some useful values
  m <- length(y) # number of training examples
  n <- ncol(X)
  J.history <- rep(0, num_iters)
  
  for(i in 1:num_iters){
    partial1 <- matrix(rep(0, n))
    partial1[1] <- (1/m) * (t(X[, 1]) %*% (sigmoid(X %*% theta) - y))
    partial1[2:n] <- (1/m) * (t(X[, 2:n]) %*% (sigmoid(X %*% theta) - y)) + 
      lambda/m * theta[2:n]
    
    partial2 <- (1/m) * (t(X) %*% (X * as.numeric(
                           (sigmoid(X %*% theta) * 
                           (1 - sigmoid(X %*% theta))))) )
    theta <- theta - ginv(partial2) %*% partial1
    #  Save the cost J in every iteration    
    J.history[i] <- (1/m) * 
      sum(-y * log(sigmoid(X %*% theta)) - 
            (1-y) * (log(1 - sigmoid(X %*% theta)))) + 
      (lambda/2/m) * sum(theta[2:n] ^2)
  }
  return(list(theta = theta, J = J.history))
}
x1 <- wmda$density
x2 <- wmda$sugar
X <- mapFeature(x1, x2, 6)
y <- as.matrix(as.numeric(as.character(wmda$label)))
initial_theta <- matrix(rep(0, 28), ncol = 1)
lambda <- 0
Ret <- HessianMatrix2(X, y, initial_theta,
                       num_iters = 10, lambda)
```

* 回归参数、代价函数曲线如下：

```{r plot11, eval=TRUE, echo=FALSE}
as.numeric(Ret$theta)
p <- sigmoid(X %*% Ret$theta)
pos <- which(p >= 0.5)
neg <- which(p < 0.5)
p[pos] <- 1
p[neg] <- 0
t <- table(p, wmda$label)
print(paste("prediction accuracy = ", round(sum(diag(t)/sum(t)), 4) * 100,
            "%"))
ggplot(data = data.frame(item = 1:length(Ret$J), J = Ret$J), 
       aes(x = item, y = J)) + geom_line()

```

### 小结
1. 对比可以发现牛顿法比梯度下降法收敛速度快的多！

2. 在最小化代价函数的过程中还有很多更高级的方法，如BFGS（共轭梯度）法、L-BGFS等，它们的优点是不用选择参数$\alpha$、收敛速度更快，但是它们也更复杂。

3. 在非线性边界画图中利用的是等值线绘图，也就是将图形分成一个个小的密度点，计算每个密度点的概率值。密度点概率值为0.5的等值线即为边界线。但是在实现过程中`geom_isobands()`并不能很好实现这个过程。Matlab可以利用函数`contour()`实现，切记在利用这个函数之前将$X$转置。

4. 在HessianMatrix矩阵的求逆过程中并没有利用`solve()`函数，而是利用了`MASS`包里的`ginv`函数，当矩阵不可逆时，这个函数求得矩阵伪逆。类似于Matlab中`inv`与`pinv`的关系。


## 线性判别分析
线性判别分析（Linear Discriminant Anaysis，简称LDA）是一种经典的线性学习方法。
LDA的思想非常朴素：给定训练集，设法将样本投射到一条直线上，使得同类样本的投影点尽可能接近、异类样本投影点尽可能远离；在对新样本进行分类时，将其投影到同样的直线上，再根据投影点的位置来确定新样本的类型。

```{r LDA, eval=TRUE, echo=FALSE}
LDA <- function(X, y){
  pos <- which(y == 1)
  neg <- which(y == 0)
  u1 <- as.matrix(colMeans(X[pos, ]))
  u0 <- as.matrix(colMeans(X[neg, ]))
  Sw <- (t(X[pos, ]) - as.numeric(u1)) %*% t(t(X[pos, ]) - as.numeric(u1)) + 
    (t(X[neg, ]) - as.numeric(u0)) %*% t(t(X[neg, ]) - as.numeric(u0))
  theta <- ginv(Sw) %*% (u0 - u1)
  return(list(u1=u1, u0=u0, theta = theta))
}

X <- as.matrix(wmda[, 2:3])
y <- as.matrix(as.numeric(as.character(wmda$label)))
Ret <- LDA(X, y)
theta <- Ret$theta
print(theta)
```

```{r plot12, eval=TRUE, echo=FALSE}
p <- ggplot(data = wmda, aes(x = density, y = sugar, colour = label)) + 
       geom_point(size = 2.0, shape = 16) + xlim(0, 0.8) + ylim(0, 0.8) +
       geom_abline(slope = (theta[2]/theta[1]), 
                   intercept = 0, color = "red")

for(i in 1:nrow(X)){
  k <- theta[2]/theta[1]
  x1 <- (X[i, 2] + X[i, 1]/k) / (k + 1/k)
  x2 <- k * x1
  da <- data.frame(x = c(X[i, 1], x1), y = c(X[i, 2], x2))
  if(y[i] == 1){
      p <- p + geom_point(x = x1, y = x2, colour = "blue", size = 2) + 
        geom_line(data = da, aes(x = x, y = y, colour = "1"))
  }else{
      p <- p + geom_point(x = x1, y = x2, colour = "red", size = 2) + 
        geom_line(data = da, aes(x = x, y = y, colour = "0"))
  }
}

p + coord_equal(ratio=1)
```

### 决策边界

```{r plot13, eval=TRUE, echo=FALSE}
u1 <- Ret$u1
u0 <- Ret$u0
k <- theta[2]/theta[1]
x11 <- (u1[2, 1] + u1[1, 1]/k) / (k + 1/k)
x21 <- k * x11
x10 <- (u0[2, 1] + u0[1, 1]/k) / (k + 1/k)
x20 <- k * x10
#
x1.u <- (x11 + x10)/2
x2.u <- (x21 + x20)/2


ggplot(data = wmda, aes(x = density, y = sugar, colour = label)) + 
  geom_point(size = 2.0, shape = 16) + xlim(0, 0.8) + ylim(0, 0.8) +
  geom_abline(slope = (theta[2]/theta[1]), intercept = 0, color = "red") +
  geom_abline(slope = -1/k, intercept = (1/k)*x1.u + x2.u, colour = "yellow", size = 1.08) + 
  coord_equal(ratio=1) + theme(legend.title=element_blank()) + 
  geom_point(aes(x = x11, y = x21, colour = "1"), size = 2.0, shape = 16) + 
  geom_point(aes(x = x10, y = x20, colour = "0"), size = 2.0, shape = 16)
```


